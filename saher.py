# -*- coding: utf-8 -*-
"""Saher.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g3RVplFLa7ykMHrZ-D9AdKVRqZM2IzFV

#📦 Step 1: Install Required Libraries
"""

!pip install opendatasets
!pip install tensorflow

"""# 📦 Step 2: Import Libraries"""

# Data handling & analysis
import opendatasets as od
import pandas as pd
import numpy as np

# Data visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Data preprocessing
from sklearn.preprocessing import MinMaxScaler,StandardScaler

from imblearn.over_sampling import ADASYN

# Train-test split
from sklearn.model_selection import train_test_split

# Machine Learning Models (Scikit-learn)
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.ensemble import BaggingClassifier

# Deep Learning (TensorFlow/Keras)
import tensorflow
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical

# Model evaluation
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import joblib
import pickle

"""#📥 Step 3: Load the Dataset"""

od.download("https://www.kaggle.com/datasets/ziya07/building-structural-health-sensor-dataset")
data = pd.read_csv("/content/building-structural-health-sensor-dataset/building_health_monitoring_dataset.csv")
data.head()

"""# 🔎 Step 4: Explore the Dataset"""

data.describe()

data.info()

data["Timestamp"] = pd.to_datetime(data["Timestamp"])
data.info()

data[data['Strain (με)'].isna()]

"""# 📊 Step 5: Visualize Data"""

data.hist(figsize=(20,15))

plt.figure(figsize=(20,15))
for i in data.select_dtypes(include="float64").columns:
  px.box(data, x=data[i], title=i).show()

for col in data.select_dtypes(include="float64").columns:
  q1, q3 = data[col].quantile([0.25, 0.75])
  iqr = q3 - q1
  lower_bound = q1 - (1.5 * iqr)
  upper_bound = q3 + (1.5 * iqr)
  print(f"{col}: {len(data[(data[col] <= lower_bound) | (data[col] >= upper_bound)])}")

"""## Fill Nulls"""

for col in data.select_dtypes(include="float64").columns:
  data[col].fillna(data[col].median(), inplace=True)
data.info()

# data.hist(figsize=(20,15))

# sns.heatmap(data.corr(numeric_only=True), annot=True)

# data

# data["total_accel"] = np.sqrt(data["Accel_X (m/s^2)"]**2 + data["Accel_Y (m/s^2)"]**2 + data["Accel_Z (m/s^2)"]**2)
# data.head()

# px.bar(data.groupby("Condition Label").size().reset_index(), x="Condition Label", y=0, title="Condition Label", labels={"0":"Count"}).show()

# for i in range(1, len(data.columns) - 1):
#   px.box(data, x=data.columns[i], title=data.columns[i]).show()

# for i in range(1, len(data.columns) - 1):
#   px.histogram(data, x=data.columns[i], title=data.columns[i]).show()

# px.scatter(data_frame=data, x="Accel_X (m/s^2)", y="Strain (με)", color="Condition Label")

# px.scatter_3d(data_frame=data, x="Accel_X (m/s^2)", y="Accel_Y (m/s^2)", z="Accel_Z (m/s^2)", color="Condition Label")

px.scatter_3d(data_frame=data, x="Accel_X (m/s^2)", y="Strain (με)", z="Temp (°C)", color="Condition Label")

accel_data = data[data.columns[1:4]]
accel_data

# data.drop(columns=data.columns[1:4], inplace=True)
# data

# data["Condition Label"].value_counts()

zero_condition_data = data.loc[data['Condition Label'] == 0].sample(n=450, random_state=42)
one_condition_data = data.loc[data['Condition Label'] == 1]
two_condition_data = data.loc[data['Condition Label'] == 2]

under_sample_data = pd.concat([zero_condition_data, one_condition_data, two_condition_data])

# Shuffle dataframe rows
under_sample_data = under_sample_data.sample(frac=1, random_state=42)
# under_sample_data["Timestamp"] = pd.to_datetime(under_sample_data["Timestamp"])


under_sample_data.head()

under_sample_data["Condition Label"].value_counts()

# X = data.drop(columns=["Condition Label", "Timestamp"])
# y = data["Condition Label"]
# adasyn = ADASYN(sampling_strategy='minority', random_state=42)
# X_resampled, y_resampled = adasyn.fit_resample(X, y)

# over_sample_data = pd.concat([X_resampled, y_resampled], axis=1)

# X = over_sample_data.drop(columns=["Condition Label"])
# y = over_sample_data["Condition Label"]
# adasyn = ADASYN(sampling_strategy='minority', random_state=42)
# X_resampled, y_resampled = adasyn.fit_resample(X, y)

# over_sample_data = pd.concat([X_resampled, y_resampled], axis=1)


# print("Oversampled data shape:", over_sample_data.shape)
# over_sample_data.head()

# over_sample_data["Condition Label"].value_counts()

# over_sample_data.info()

# over_sample_data.hist(figsize=(20,15))

# data.drop(columns=["Timestamp"], inplace=True)

under_sample_data.drop(columns=["Timestamp"], inplace=True)

# data.tail()

# data.hist(figsize=(20,15))

# x = data.drop(columns=["Condition Label"])
# y = data["Condition Label"]
# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)
# x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42, stratify=y_test)
# scaler = MinMaxScaler()
# x_train = scaler.fit_transform(x_train)
# x_test = scaler.transform(x_test)
# x_val = scaler.transform(x_val)

"""dataTotal.hist(figsize=(20,15))"""

# data.head()

# sns.heatmap(data.corr(numeric_only=True), annot=True)

"""to download the clean data run this code"""

# data.to_csv("processed_data.csv", index=False)
# from google.colab import files
# files.download("processed_data.csv")

"""# Modeling

## 1-Modeling with normal data
"""

# LR = LogisticRegression(class_weight="balanced")
# LR.fit(x_train, y_train)
# y_pred = LR.predict(x_val)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# DT = DecisionTreeClassifier(class_weight="balanced",max_depth=3)
# DT.fit(x_train, y_train)
# y_pred = DT.predict(x_val)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# max_depth = [None,1,2,3,4,5,6,7,8,9,10]
# features = [1,2,3,4,5]

# best_acc = 0
# best_params = {}
# best_report = None
# best_matrix = None

# for i in max_depth:
#     for j in features:
#         DT = DecisionTreeClassifier(class_weight="balanced",
#                                     max_depth=i,
#                                     max_features=j,
#                                     random_state=42)
#         DT.fit(x_train, y_train)
#         y_pred = DT.predict(x_val)

#         acc = accuracy_score(y_val, y_pred)

#         if acc > best_acc:
#             best_acc = acc
#             best_params = {"max_depth": i, "max_features": j}
#             best_report = classification_report(y_val, y_pred)
#             best_matrix = confusion_matrix(y_val, y_pred)

# print("Parameters:", best_params)
# print("Accuracy:", best_acc)
# print("Confusion Matrix:\n", best_matrix)
# print("Classification Report:\n", best_report)

# RF = RandomForestClassifier(class_weight="balanced", n_estimators=100, max_depth=3)
# RF.fit(x_train,y_train)
# y_pred = RF.predict(x_val)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# knn = KNeighborsClassifier(n_neighbors=9)
# knn.fit(x_train, y_train)
# y_pred = knn.predict(x_val)
# knn.predict(x_val)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# svm = SVC(class_weight="balanced", C=1)
# svm.fit(x_train, y_train)
# y_pred = svm.predict(x_val)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# XG = XGBClassifier(class_weight="balanced", n_estimators=100, max_depth=3)
# XG.fit(x_train, y_train)
# y_pred = XG.predict(x_val)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# naive_bayes = GaussianNB( )
# naive_bayes.fit(x_train, y_train)
# y_pred = naive_bayes.predict(x_val)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# callbacks = [EarlyStopping(monitor='val_loss', patience=15,restore_best_weights=True)]
# model = Sequential()
# model.add(Dense(512, activation='relu', input_shape=(x_train.shape[1],)))
# model.add(Dense(256, activation='relu'))
# model.add(Dense(64, activation='relu'))
# model.add(Dense(32, activation='relu'))
# model.add(Dense(3, activation='softmax'))
# model.compile(optimizer='adamW', loss='categorical_crossentropy', metrics=['accuracy'])
# history = model.fit(x_train, to_categorical(y_train), epochs=500, batch_size=32, validation_data=(x_val, to_categorical(y_val)),callbacks=callbacks)

# y_pred = model.predict(x_val)
# y_pred = np.argmax(y_pred, axis=1)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
# plt.title('Model loss')
# plt.ylabel('Loss')
# plt.xlabel('Epoch')
# plt.legend(['Train', 'Validation'], loc='upper right')
# plt.show()
# plt.plot(history.history['accuracy'])
# plt.plot(history.history['val_accuracy'])
# plt.title('Model accuracy')
# plt.ylabel('Accuracy')
# plt.xlabel('Epoch')
# plt.legend(['Train', 'Validation'], loc='lower right')
# plt.show()

"""## 2-Modeling with over sampled data"""

# x = over_sample_data.drop(columns=["Condition Label"])
# y = over_sample_data["Condition Label"]
# x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42,stratify=y)
# x_test,x_val,y_test,y_val = train_test_split(x_test,y_test,test_size=0.5,random_state=42,stratify=y_test)

# x_train.head()

# LR = LogisticRegression( )
# LR.fit(x_train, y_train)
# y_pred = LR.predict(x_val)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# DT = DecisionTreeClassifier(class_weight="balanced",max_depth=3)
# DT.fit(x_train, y_train)
# y_pred = DT.predict(x_val)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# max_depth = [None,1,2,3,4,5,6,7,8,9,10]
# features = [1,2,3,4,5]

# best_acc = 0
# best_params = {}
# best_report = None
# best_matrix = None

# for i in max_depth:
#     for j in features:
#         DT = DecisionTreeClassifier(class_weight="balanced",
#                                     max_depth=i,
#                                     max_features=j,
#                                     random_state=42)
#         DT.fit(x_train, y_train)
#         y_pred = DT.predict(x_val)

#         acc = accuracy_score(y_val, y_pred)

#         if acc > best_acc:
#             best_acc = acc
#             best_params = {"max_depth": i, "max_features": j}
#             best_report = classification_report(y_val, y_pred)
#             best_matrix = confusion_matrix(y_val, y_pred)

# print("Parameters:", best_params)
# print("Accuracy:", best_acc)
# print("Confusion Matrix:\n", best_matrix)
# print("Classification Report:\n", best_report)

# RF = RandomForestClassifier(class_weight="balanced", n_estimators=100, max_depth=3)
# RF.fit(x_train,y_train)
# y_pred = RF.predict(x_val)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# knn = KNeighborsClassifier(n_neighbors=9)
# knn.fit(x_train, y_train)
# y_pred = knn.predict(x_val)
# knn.predict(x_val)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# svm = SVC(class_weight="balanced", C=1)
# svm.fit(x_train, y_train)
# y_pred = svm.predict(x_val)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# XG = XGBClassifier(class_weight="balanced", n_estimators=100, max_depth=3)
# XG.fit(x_train, y_train)
# y_pred = XG.predict(x_val)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# naive_bayes = GaussianNB()
# naive_bayes.fit(x_train, y_train)
# y_pred = naive_bayes.predict(x_val)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# callbacks = [EarlyStopping(monitor='val_loss', patience=15,restore_best_weights=True)]
# model = Sequential()
# model.add(Dense(1024, activation='relu', input_shape=(x_train.shape[1],)))
# model.add(Dense(512, activation='relu'))
# model.add(Dense(256, activation='relu'))
# model.add(Dense(128, activation='relu'))
# model.add(Dense(64, activation='relu'))
# model.add(Dense(32, activation='relu'))
# model.add(Dense(3, activation='softmax'))
# model.compile(optimizer='adamW', loss='categorical_crossentropy', metrics=['accuracy'])
# history = model.fit(x_train, to_categorical(y_train), epochs=500, batch_size=32, validation_data=(x_val, to_categorical(y_val)),callbacks=callbacks)

# y_pred = model.predict(x_val)
# y_pred = np.argmax(y_pred, axis=1)
# print(accuracy_score(y_val, y_pred))
# print(confusion_matrix(y_val, y_pred))
# print(classification_report(y_val, y_pred))

# model.save("saher1.h5")

# plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
# plt.title('Model loss')
# plt.ylabel('Loss')
# plt.xlabel('Epoch')
# plt.legend(['Train', 'Validation'], loc='upper right')
# plt.show()
# plt.plot(history.history['accuracy'])
# plt.plot(history.history['val_accuracy'])
# plt.title('Model accuracy')
# plt.ylabel('Accuracy')
# plt.xlabel('Epoch')
# plt.legend(['Train', 'Validation'], loc='lower right')
# plt.show()

"""## 3-Modeling with under sampled data"""

x = under_sample_data.drop(columns=["Condition Label"])
y = under_sample_data["Condition Label"]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)
x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42, stratify=y_test)
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_val   = scaler.transform(x_val)
x_test  = scaler.transform(x_test)
with open("minmax_scaler.pkl", "wb") as f:
    pickle.dump(scaler, f)

DT = DecisionTreeClassifier( max_depth=4)
DT.fit(x_train, y_train)
y_pred = DT.predict(x_val)
print(accuracy_score(y_val, y_pred))
print(confusion_matrix(y_val, y_pred))
print(classification_report(y_val, y_pred))

max_depth = [None,1,2,3,4,5,6,7,8,9,10]
features = [1,2,3,4,5]

best_acc = 0
best_params = {}
best_report = None
best_matrix = None
best_model = None   # ⬅️ هنا هنخزن أفضل موديل

for i in max_depth:
    for j in features:
        BDT = DecisionTreeClassifier(class_weight="balanced",
                                    max_depth=i,
                                    max_features=j,
                                    random_state=42)
        BDT.fit(x_train, y_train)
        y_pred = BDT.predict(x_val)

        acc = accuracy_score(y_val, y_pred)

        if acc > best_acc:
            best_acc = acc
            best_params = {"max_depth": i, "max_features": j}
            best_report = classification_report(y_val, y_pred)
            best_matrix = confusion_matrix(y_val, y_pred)
            best_model = BDT   # ⬅️ خزّنا نسخة من أفضل موديل

print("Parameters:", best_params)
print("Accuracy:", best_acc)
print("Confusion Matrix:\n", best_matrix)
print("Classification Report:\n", best_report)

# 🔹 حفظ أفضل موديل في ملف pkl
joblib.dump(best_model, "best_decision_tree.pkl")
print("✅ Best model saved as best_decision_tree.pkl")

RF = RandomForestClassifier(class_weight="balanced", n_estimators=100, max_depth=3)
RF.fit(x_train,y_train)
y_pred = RF.predict(x_val)
print(accuracy_score(y_val, y_pred))
print(confusion_matrix(y_val, y_pred))
print(classification_report(y_val, y_pred))

knn = KNeighborsClassifier(n_neighbors=9)
knn.fit(x_train, y_train)
y_pred = knn.predict(x_val)
knn.predict(x_val)
print(accuracy_score(y_val, y_pred))
print(confusion_matrix(y_val, y_pred))
print(classification_report(y_val, y_pred))

svm = SVC(class_weight="balanced", C=1)
svm.fit(x_train, y_train)
y_pred = svm.predict(x_val)
print(accuracy_score(y_val, y_pred))
print(confusion_matrix(y_val, y_pred))
print(classification_report(y_val, y_pred))

XG = XGBClassifier(class_weight="balanced", n_estimators=100, max_depth=3)
XG.fit(x_train, y_train)
y_pred = XG.predict(x_val)
print(accuracy_score(y_val, y_pred))
print(confusion_matrix(y_val, y_pred))
print(classification_report(y_val, y_pred))

ada = AdaBoostClassifier()
ada.fit(x_train, y_train)
y_pred = ada.predict(x_val)
print(accuracy_score(y_val, y_pred))
print(confusion_matrix(y_val, y_pred))
print(classification_report(y_val, y_pred))

naive_bayes = GaussianNB()
naive_bayes.fit(x_train, y_train)
y_pred = naive_bayes.predict(x_val)
print(accuracy_score(y_val, y_pred))
print(confusion_matrix(y_val, y_pred))
print(classification_report(y_val, y_pred))

callbacks = [EarlyStopping(monitor='val_loss', patience=15,restore_best_weights=True)]
model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(x_train.shape[1],)))
model.add(Dense(256, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(3, activation='softmax'))
model.compile(optimizer='adamW', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, to_categorical(y_train), epochs=500, batch_size=32, validation_data=(x_val, to_categorical(y_val)),callbacks=callbacks)

y_pred = model.predict(x_val)
y_pred = np.argmax(y_pred, axis=1)
print(accuracy_score(y_val, y_pred))
print(confusion_matrix(y_val, y_pred))
print(classification_report(y_val, y_pred))

model.save("saher2.h5")

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='lower right')
plt.show()

under_sample_data.head()

data.head()